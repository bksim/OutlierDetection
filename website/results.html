<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html lang="en"> <!--<![endif]-->
<head>

	<!-- Basic Page Needs
  ================================================== -->
	<meta charset="utf-8">
	<title>Outlier Detection</title>
	<meta name="description" content="">
	<meta name="author" content="">

	<!-- Mobile Specific Metas
  ================================================== -->
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<!-- CSS
  ================================================== -->
	<link rel="stylesheet" href="stylesheets/base.css">
	<link rel="stylesheet" href="stylesheets/skeleton.css">
	<link rel="stylesheet" href="stylesheets/layout.css">
	<link rel="stylesheet" href="stylesheets/default.css">


	<!--[if lt IE 9]>
		<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->

	<script type="text/x-mathjax-config">
	  MathJax.Hub.Config({
	    tex2jax: {
	      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
	      processEscapes: true
	    }
	  });
	</script>
	<script type="text/javascript"
	    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>

	<!-- Favicons
	================================================== -->
	<link rel="shortcut icon" href="images/favicon.ico">
	<link rel="apple-touch-icon" href="images/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">

	<script src="js/jquery-1.11.0.min.js"></script>
	<script src="js/lightbox.min.js"></script>
	<link href="css/lightbox.css" rel="stylesheet" />

</head>
<body>



	<!-- Primary Page Layout
	================================================== -->

	<!-- Delete everything in this .container and get started on your own site! -->

	<!--<div class="container">
		<div class="sixteen columns">
			<h1 class="remove-bottom" style="margin-top: 40px">Skeleton</h1>
			<h5>Version 1.2</h5>
			<hr />
		</div>
		<div class="one-third column">
			<h3>About Skeleton?</h3>
			<p>Skeleton is a small collection of well-organized CSS files that can help you rapidly develop sites that look beautiful at any size, be it a 17" laptop screen or an iPhone. It's based on a responsive grid, but also provides very basic CSS for typography, buttons, forms and media queries. Go ahead, resize this super basic page to see the grid in action.</p>
		</div>
		<div class="one-third column">
			<h3>Three Core Principles</h3>
			<p>Skeleton is built on three core principles:</p>
			<ul class="square">
				<li><strong>A Responsive Grid Down To Mobile</strong>: Elegant scaling from a browser to tablets to mobile.</li>
				<li><strong>Fast to Start</strong>: It's a tool for rapid development with best practices</li>
				<li><strong>Style Agnostic</strong>: It provides the most basic, beautiful styles, but is meant to be overwritten.</li>
			</ul>
		</div>
		<div class="one-third column">
			<h3>Docs &amp; Support</h3>
			<p>The easiest way to really get started with Skeleton is to check out the full docs and info at <a href="http://www.getskeleton.com">www.getskeleton.com.</a>. Skeleton is also open-source and has a <a href="https://github.com/dhgamache/skeleton">project on git</a>, so check that out if you want to report bugs or create a pull request. If you have any questions, thoughts, concerns or feedback, please don't hesitate to email me at <a href="mailto:hi@getskeleton.com">hi@getskeleton.com</a>.</p>
		</div>

	</div>-->
	<!-- container -->

	<div class="main container">
		<div class="row">
			<div class="three columns sidebar">
				<div id="nav">
					<h4 class="hero">Navigation</h4>
					<a href="./index.html">Home</a>
					<a href="./motivation.html">Motivation</a>
					<a href="./data.html">Data &amp; Initial Exploration</a>
					<a href="./methods.html">Previous Work &amp; Methods</a>
					<a href="./results.html">Mixture of Experts &amp; Results</a>
				</div>
			</div>

			<div id="content" class="nine columns">
				<h4 class="title">Mixture of Experts &amp; Results</h4>
				<h5>Introduction to the Mixture of Experts</h5>
				<p>With the mixture of experts approach, we assume that each outlier detection method performs best within a particular domain of the sample space. In this ensemble method, we combine the results from each method in a smart way so that the diversity of experts can make up for deficiencies in individual methods over particular domains. Therefore, the result of each expert is weighted by values generated based on the location of the point in the 57-dimensional space. The gating parameter $\eta_i$ for each expert $i$ must be trained. </p>

				<p>In this project, six outlier detection methods (see Methods section) were used on either the infrared (I) or visual (V) channels of the light curves. This gave a total of <strong>twelve experts</strong> (six methods on two different channels). Here, we describe each outlier detection method and show its stand-alone performance.</p>

				<img src="images/Results0.png" width="50%" style="margin-left: auto; margin-right: auto; display: block;margin-bottom:15px;"/>
				<p class="caption"><strong>Figure R1. Mixture of Experts. </strong> The result of each expert is weighted by values generated from a gating network. Both the gating network and the experts must be trained.</p>

				<p>The gating probability $g_i^x$ is the weight assigned to each expert $i$ for data point $\vec{x}$. The weights are generated using a soft-max gating network:
					$$g_i^x= \frac{exp(\vec{\eta_i}^T \vec{x})}{\sum_{j=1}^k exp(\vec{\eta_j}^T \vec{x}) },$$
					where $\vec{x}$ is a 57-dimensional data vector (one data point), and $\eta_i$ is the gating parameter for each expert $i$, with a total of $k$ experts. Thus, the outlierliness score $P_x$ for a particular data point $\vec{x}$ is:
					$$P_x = \sum_{i=1}^k g_i(\vec{\eta_j},\vec{x}) p_i(\vec{x}),$$
					where $p_i(\vec{x})$ is the outlierliness score assigned to a particular data point $\vec{x}$ by expert/model $i$.
				</p>


				<h5>Results</h5>
				<p>A mixture of experts model was trained on the data set of 1913 objects, optimizing for the detection of the artificial and the rcb group outliers. The optimization was done using simulated annealing over the entire range of possible $\eta$ values.</p>
				<a href='images/Results1.png' data-lightbox='Results1'>
				<img src="images/Results1.png" width="70%" style="margin-left: auto; margin-right: auto; display: block;margin-bottom:15px;"/></a>
				<p class="caption"><strong>Figure R2. ROC for the combined mixture of experts outlier detection method. </strong> The combined model is able to perform just as well as the best individual expert in the detection of artificial outliers. It performs comparably to the best individual expert in the detection of rcb class outliers.</p>

				<p>As shown above, the mixture of experts ensemble method is able to perform as well as the best individual outlier detection method for the artificial outliers. For the rcb class outliers, it does similarly to the top individual outlier detection methods. We cannot a priori expect the mixture to perform better than all the individual experts for both types of outlier detection (artificial and rcb) because we optimized <em>both</em> the rcb and the artificial outlier detection. The simultaneous optimization of two responses may have lead to compromises in the absolute performance for each specific outlier class. </p>

				<p>Table R1 shows the performance as measured by area under the ROC curve (AUC) for each expert along with the performance of the mixture of experts. Again, since the mixture of experts is optimized for both artificial and rcb class outliers, it seems intuitive that the performance of detecting any <strong>one</strong> type of outlier may may not be as high as that for individual experts. However, it makes sense that the average AUC for the mixture of experts is highest. The mixture method still seems to be able to maintain high performance for the artificial outlier detection. </p>

					<table>
					  <thead>
					    <tr>
					      <th>Method</th>
					      <th>Artificial Outliers AUC</th>
					      <th>RCB Outliers AUC</th>
					      <th>Average AUC</th>
					    </tr>
					  </thead>
					  <tbody>
					    <tr>
					      <td>KNN 1 I-channel</td>
					      <td>0.85</td>
					      <td>0.94</td>
					      <td>0.90</td>
					    </tr>
					    <tr>
					      <td>KNN 1 V-channel</td>
					      <td>0.42</td>
					      <td>0.88</td>
					      <td>0.65</td>
					    </tr>
					    <tr>
					      <td>KNN 2 I-channel</td>
					      <td>0.50</td>
					      <td><strong>0.95</strong></td>
					      <td>0.73</td>
					    </tr>
					    <tr>
					      <td>KNN 2 V-channel</td>
					      <td>0.50</td>
					      <td>0.68</td>
					      <td>0.59</td>
					    </tr>
					    <tr>
					      <td>SVM+JP I-channel</td>
					      <td>0.78</td>
					      <td>0.91</td>
					      <td>0.85</td>
					    </tr>
					    <tr>
					      <td>SVM+JP V-channel</td>
					      <td>0.62</td>
					      <td>0.73</td>
					      <td>0.68</td>
					    </tr>
					    <tr>
					      <td>LOCI I-channel</td>
					      <td>0.30</td>
					      <td>0.28</td>
					      <td>0.29</td>
					    </tr>
					    <tr>
					      <td>LOCI V-channel</td>
					      <td>0.28</td>
					      <td>0.45</td>
					      <td>0.37</td>
					    </tr>
					    <tr>
					      <td>Hyperplane I-channel</td>
					      <td>0.78</td>
					      <td>0.78</td>
					      <td>0.78</td>
					    </tr>
					    <tr>
					      <td>Hyperplane V-channel</td>
					      <td><strong>0.99</strong></td>
					      <td>0.82</td>
					      <td><strong>0.91</strong></td>
					    </tr>
					    <tr>
					      <td>Mixture of Experts</td>
					      <td>0.99</td>
					      <td>0.89</td>
					      <td>0.94</td>
					    </tr>
					   </tbody>
					</table>
				<p class="caption"><strong>Table R1. Area under the ROC curve (AUC) for various outlier detection methods.</strong> The AUC is a common single-number metric to compare between ROC curves (such as those seen in the Methods section). Bolded AUC values are from the individual outlier detection methods that did the best under each outlier detection task (either detecting artificial outliers or the rcb class as outliers). </p>

				<p>To visualize the 57-dimension data points more easily, principal component analysis (PCA) was done on the extracted time series features for each light curve. The first two principal components were used to make a 2D plot (Figure D1). PCA was used because visualizing a 57-dimensional space on a 2D image is very difficult. One could imagine plotting the data points on axes of each pair of features. However, this would create too many (57 choose 2) plots and therefore would be hard to easily grasp the shape of the data point cloud. PCA is a great way to draw a plane through the 57-dimensional data point cloud in such a way that the plane captures the most variation in the data. Projecting the data points on this place thus may allow visualization of outlier points more easily. We note that PCA was solely used as a visualization tool, and was not used for any of the mathematical mechanics in building the outlier detection models.</p>

				<a href='images/Results2.png' data-lightbox='Results2'>
				<img src="images/Results2.png" width="70%" style="margin-left: auto; margin-right: auto; display: block;margin-bottom:15px;"/></a>
				<p class="caption"><strong>Figure R3. Data points and detected outliers on the PCA plane (I-channel).</strong> Detected outliers are outlied in yellow. The threshold 1.1 was chosen by eye such that the obvious outliers were identified, allowing only about 1% false positive rate. The false positive rate at this threshold is 1% (231 non-rcb objects) and the true postive rate is 68% (18/22 rcb objects).</p>

				<p>As can be seen in Figure R3, the mixture of experts model is able to identify almost all of the objects in class rcb that appear to be far away from the other points on the PCA plane. With an arbitrary threshold (1.1) the false positive rate is 1% (231 non-rcb objects) and the true postive rate is 68% (18/22 rcb objects). Of course, depending on one's tolerance for type 1 or type 2 error, the threshold can be adjusted. </p>



				<h5>Application to Larger Data Set</h5>
				<p>We used the mixture of experts model trained with the 1913 outliers to detect outliers in the larger data set of 49006 objects (a total of 98012 light curves accounting for I and V channel): 136 from dpv, 14883 from lpv, 1252 from dsct, 15377 from ecl, 202 from t2cep, 2139 from cep, 82 from acep, 14913 from rrlyr, 22 from rcb. This set includes the training set. Note that the number of rcb class outliers is the same as the training set, so this exercise will show the performance of the mixture model in identifying the rcb class while dealing with more data. There is also an additional dpv class. The ROC curve displaying the results are shown in figure R4.</p>

				<p>To test the robustness of the model, the mixture of experts model parameter $\eta$ was additionally trained with two different small samples of 2000. Thus, to summarize, $\eta 1$ is was trained on the set of 1913 objects described above. $\eta 2$ and $\eta 3$ were trained on 2000 randomly selected objects. Since the rcb class outliers were used to help train the model, we always include the 22 rcb objects, but the other 1978 objects were randomly chosen from our larger data set of 49006 objects (excluding the rcb objects). That is, each object in the 49006 data set had equal probability of being selected (if they were not rcb, as rcb objects automatically get selected) and we only selected 1978 of them. These are just bootstraped data points, so we did not have to define any range in sample space. For $\eta 2$, the distribution of the 2000 points used to train the mixture of expert parameter $\eta$ were: 6 from dpv, 576 from lpv, 65 from dsct, 656 from ecl, 6 from t2cep, 77 from cep, 3 from acep, 589 from rrlyr, 22 from rcb. For $\eta 3$, the distribution of the 2000 points used to train the mixture of expert parameter $\eta$ were: 5 from dpv, 605 from lpv, 46 from dsct, 610 from ecl, 10 from t2cep, 85 from cep, 3 from acep, 614 from rrlyr, 22 from rcb. Note that the distribution of points from each class are different between the three mixture of experts training sets. </p>

				<a href='images/Results3.png' data-lightbox='Results3'>
				<img src="images/Results3.png" width="70%" style="margin-left: auto; margin-right: auto; display: block;margin-bottom:15px;"/></a>
				<p class="caption"><strong>Figure R4. ROC for the mixture of experts outlier detection method applied to larger dataset of 49006 objects. </strong> The mixture of experts model trained on the smaller data set does well in this scenario in identifying the rcb class as outliers. The three lines show the results from three mixture of experts models (3 different $\eta$ parameters) that were trained on different small sample sizes. </p>

				<p>As Figure R4 shows, the mixture of experts model is readily applicable to larger data sets. It does fairly well in the detection of rcb class outliers. It is robust to different small training sets, even with a different distribution of data points. </p>

				<a href='images/Results4.png' data-lightbox='Results4'>
				<img src="images/Results4.png" width="70%" style="margin-left: auto; margin-right: auto; display: block;margin-bottom:15px;"/></a>
				<p class="caption"><strong>Figure R5. Large data set with detected outliers plotted on the PCA plane (I-channel).</strong> Outliers detected by the mixture of experts model with parameter $\eta 3$ are outlied in yellow. The top 10 outliers detected by the mixture of experts model with parameter $\eta 3$ are outlied in red. The threshold 1.2 was chosen such that the false positive rate (as defined in the rcb class outlier detection context) was around 1%. The false positive rate at this threshold is 1% (505 non-rcb class objects) and the true postive rate is 36.4% (8/22 rcb class objects).</p>

				<p>As can be seen in Figure R5, the mixture of experts model is able to identify almost all of the objects that appear to be far away from the other points on the PCA plane, even though most of them are not in the class rcb. This suggests that our current ROC curve (which measures effectiveness in identifying rcb class outliers) may not be the best metric to evalate these models. It also shows that this outlier detection method is very applicable to larger data sets in identifying outliers in classes that we didn't even train with. </p>



				<p>The top ten outliers identified were the following: 2 from the class RRLYR, 2 from the class RCB, and 6 from the class LPV. In the following figures, we plot the outlier timeseries (in red) and we plot the rest of the timeseries in its class (in gray). </p>
				<a href='outlier_images/lpv_I.png' data-lightbox='lpv_I'>
				<img src="outlier_images/lpv_I.png" width="70%" style="margin-left: auto; margin-right: auto; display: block;margin-bottom:15px;"/></a>
				<a href='outlier_images/lpv_V.png' data-lightbox='lpv_V'>
				<img src="outlier_images/lpv_V.png" width="70%" style="margin-left: auto; margin-right: auto; display: block;margin-bottom:15px;"/></a>

				<p class="caption"><strong>Figure R6. Outlier and typical lightcurve timeseries from class LPV.</strong> Timeseries belonging to objects identified as part of the top ten outliers are colored in red. Other timeseries that are in the same class are colored in gray. It seems obvious that the red timeseries are different from the others. Some of them have lower magnitude, while some are longer. Note that objects are flagged as outlier using information from both the I and the V channels combined. Thus, while some red timeseries may not appear to be outlier, outlierliness in the <em>combination</em> of that object's I and V channels may have caused the object to be marked as outlier. </p>

				<a href='outlier_images/rcb_I.png' data-lightbox='rcb_I'>
				<img src="outlier_images/rcb_I.png" width="70%" style="margin-left: auto; margin-right: auto; display: block;margin-bottom:15px;"/></a>
				<a href='outlier_images/rcb_V.png' data-lightbox='rcb_V'>
				<img src="outlier_images/rcb_V.png" width="70%" style="margin-left: auto; margin-right: auto; display: block;margin-bottom:15px;"/></a>

				<p class="caption"><strong>Figure R7. Outlier and typical lightcurve timeseries from class RCB.</strong> Timeseries belonging to objects identified as part of the top ten outliers are colored in red. Other timeseries that are in the same class are colored in gray. Note that objects are flagged as outlier using information from both the I and the V channels combined. Thus, while some red timeseries may not appear to be outlier, outlierliness in the <em>combination</em> of that object's I and V channels may have caused the object to be marked as outlier. </p>

				<a href='outlier_images/rrlyr_I.png' data-lightbox='rrlyr_I'>
				<img src="outlier_images/rrlyr_I.png" width="70%" style="margin-left: auto; margin-right: auto; display: block;margin-bottom:15px;"/></a>
				<a href='outlier_images/rrlyr_V.png' data-lightbox='rrlyr_V'>
				<img src="outlier_images/rrlyr_V.png" width="70%" style="margin-left: auto; margin-right: auto; display: block;margin-bottom:15px;"/></a>

				<p class="caption"><strong>Figure R8. Outlier and typical lightcurve timeseries from class RRLYR.</strong> Timeseries belonging to objects identified as part of the top ten outliers are colored in red. Other timeseries that are in the same class are colored in gray. It seems obvious that the red timeseries are different from the others, just due to length. Note that objects are flagged as outlier using information from both the I and the V channels combined. Thus, while some red timeseries may not appear to be outlier, outlierliness in the <em>combination</em> of that object's I and V channels may have caused the object to be marked as outlier. </p>


				<h5>Next Steps</h5>
				<p>Future work should focus on the addition of more outlier detection "experts" to the model. In addition, the optimization of the gating parmeter $\eta$ in mixture of experts model may be optimized simultaneously with individual expert parameters using expectation maximization and stochastic gradient descent. In addition, we look to use our easily extensible code to extend our work to other domains - in particular, fraud detection in financial transaction networks.</p>
			</div>
		</div>
	</div><!-- container -->

<!-- End Document
================================================== -->
</body>
</html>